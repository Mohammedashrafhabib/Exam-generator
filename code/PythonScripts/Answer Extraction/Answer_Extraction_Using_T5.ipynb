{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf9plwkRCtJF"
      },
      "source": [
        "# Package Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opCkNO35Bcyq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyZuHcdH6GNo"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u71WAZuOF8O8"
      },
      "source": [
        "# QG Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1lkEgQ2qGLYD"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Twq9AmGDPM"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from datasets import Features, Value, Sequence\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
        "import datetime\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9d0G5gXZEzv0"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycrqawCUE3wz"
      },
      "outputs": [],
      "source": [
        "task_prefix = 'extract answers: '\n",
        "learning_rate = 3e-4\n",
        "encoder_max_len = 250\n",
        "decoder_max_len = 70\n",
        "batch_size = 4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5UgN1eJOFf4F"
      },
      "source": [
        "## Tokenizer & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIZeOxPeFn1t"
      },
      "outputs": [],
      "source": [
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pPBs0tTUGNUO"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GM1X7HIdgAXs"
      },
      "source": [
        "### Check if the datasets are already saved on Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4-Zb-oq3Tsb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "train_ds_file = Path(r'/content/drive/MyDrive/Datasets/AnswerExtraction/squad-valid-encoded.json')\n",
        "is_ds_saved = False\n",
        "if train_ds_file.is_file():\n",
        "    is_ds_saved = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FXMgiMq9oiCP"
      },
      "source": [
        "### Case 1: Datasets **NOT** saved on Drive\n",
        "-----\n",
        "**Preprocessing and encoding datasets**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xEic2hRcoYAS"
      },
      "source": [
        "#### NewsQA Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-fx0Vy161OcS"
      },
      "source": [
        "**Downloading and loading dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OISnCQyKUfy"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  !gdown --folder 1ujEc3UsU73RakkZ9RSYnlfbTyIcyRUAm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuYa6ggU56V3"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  newsqa_dataset_dir = \"/content/NewsQaDataset\"\n",
        "\n",
        "  train=load_dataset(\"newsqa\", split=\"train[:90%]\",data_dir=newsqa_dataset_dir,name=\"combined-json\")\n",
        "  validation=load_dataset(\"newsqa\", split=\"train[-10%:]\",data_dir=newsqa_dataset_dir,name=\"combined-json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnhR6SzWVC-7"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  print(next(iter(train)))\n",
        "  print(next(iter(validation)))\n",
        "  print(len(train))\n",
        "  print(len(validation))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ovXePODvzdWd"
      },
      "source": [
        "**Get split counts (train, dev/validation, test)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2fDnpNyxrH8"
      },
      "outputs": [],
      "source": [
        "# newsqa_dataset_dir = \"/content/NewsQaDataset\"\n",
        "\n",
        "# tmp = load_dataset(\"newsqa\", split='train', data_dir=newsqa_dataset_dir, name=\"combined-json\")\n",
        "\n",
        "# train_count = 0\n",
        "# validation_count = 0\n",
        "# test_count = 0\n",
        "\n",
        "# for example in tmp:\n",
        "#   if(example['type'] == 'train'):\n",
        "#     train_count += 1\n",
        "#   elif(example['type'] == 'dev'):\n",
        "#     validation_count += 1\n",
        "#   elif(example['type'] == 'test'):\n",
        "#     test_count += 1\n",
        "#   else:\n",
        "#     print(example['type'])\n",
        "\n",
        "# print(train_count)\n",
        "# print(validation_count)\n",
        "# print(test_count)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AoQuEX2x1Teh"
      },
      "source": [
        "**Remove unused columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN7Se93QtgoA"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  train_tmp = train.remove_columns(['storyId','type'])\n",
        "  train_tmp = train_tmp.flatten()\n",
        "  train_tmp = train_tmp.remove_columns(['questions.q','questions.isAnswerAbsent', 'questions.isQuestionBad','questions.answers', 'questions.validated_answers'])\n",
        "\n",
        "  train_tmp = train_tmp.rename_column(\"text\", \"context\")\n",
        "  train_tmp = train_tmp.rename_column(\"questions.consensus\", \"answers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INbTIHVxdivy"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  validation_tmp = validation.remove_columns(['storyId','type'])\n",
        "  validation_tmp = validation_tmp.flatten()\n",
        "  validation_tmp = validation_tmp.remove_columns(['questions.q','questions.isAnswerAbsent', 'questions.isQuestionBad','questions.answers', 'questions.validated_answers'])\n",
        "\n",
        "  validation_tmp = validation_tmp.rename_column(\"text\", \"context\")\n",
        "  validation_tmp = validation_tmp.rename_column(\"questions.consensus\", \"answers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhJry_TCgSJl"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  print(train_tmp.features)\n",
        "  print(validation_tmp.features)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jwEkcfoz1Yb2"
      },
      "source": [
        "**Remove duplicates, leading and trailing whitespace, and punctuation from answers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjcJN4GAiw2a"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from string import punctuation\n",
        "\n",
        "#punctuation = !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "# we will remove the dollar sign $ ------ assssssskkkkkkk\n",
        "\n",
        "def answersToList(example):\n",
        "  answersSet = set()\n",
        "  for answer in example['answers']:\n",
        "    if answer['noAnswer'] or answer['badQuestion']:\n",
        "      continue\n",
        "    answerstring = example['context'][answer['s']:answer['e']]\n",
        "    answerstring = answerstring.strip()\n",
        "    answerstring = answerstring.strip(punctuation)\n",
        "    answersSet.add(answerstring)\n",
        "  answersList=list(answersSet)\n",
        "  example['answers']=answersList\n",
        "  return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFDJphDWgVWk"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  newsqa_train_ds = train_tmp.map(answersToList, num_proc=16)\n",
        "  newsqa_valid_ds = validation_tmp.map(answersToList, num_proc=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGmPe2PliALH"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  print(next(iter(newsqa_train_ds)))\n",
        "  print(next(iter(newsqa_valid_ds)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HQsPXXXVJAJ5"
      },
      "source": [
        "**Remove rows with no answers**\n",
        "\n",
        "> 20 in train and 4 in validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TiA7TWkFVIN"
      },
      "outputs": [],
      "source": [
        "def getNoAnswerIndices(ds):\n",
        "  no_answer_indices = list()\n",
        "  for index, example in enumerate(ds):\n",
        "    # all() returns True if condition is true for all elements OR list is empty\n",
        "    if(all(answer == '' for answer in example['answers'])):\n",
        "      no_answer_indices.append(index)\n",
        "  return no_answer_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LxT6PIi5XG3"
      },
      "outputs": [],
      "source": [
        "def removeNoAnswerIndices(ds, no_answer_indices):\n",
        "  all_indices = list(range(0, len(ds)))\n",
        "  filtered_indices = [x for x in all_indices if x not in no_answer_indices]\n",
        "  filtered_ds = ds.select(filtered_indices)\n",
        "  return filtered_ds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JxV4kfm78TPi"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRm4hxYfHXSA"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  no_answer_indices = getNoAnswerIndices(newsqa_train_ds)\n",
        "  print(len(no_answer_indices))\n",
        "\n",
        "  # for index in no_answer_indices:\n",
        "  #   print(newsqa_train_ds[index]['answers'])\n",
        "\n",
        "  # for index in no_answer_indices:\n",
        "  #   print(train[index]['questions']['consensus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pIPnGwi5qVr"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  len_before = len(newsqa_train_ds)\n",
        "  newsqa_train_ds = removeNoAnswerIndices(newsqa_train_ds, no_answer_indices)\n",
        "  len_after = len(newsqa_train_ds)\n",
        "  print(f'Removed {len_before - len_after}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2HZH8e9x8UW_"
      },
      "source": [
        "Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajWI8q8iz-P3"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  no_answer_indices = getNoAnswerIndices(newsqa_valid_ds)\n",
        "  print(len(no_answer_indices))\n",
        "\n",
        "  # for index in no_answer_indices:\n",
        "  #   print(newsqa_valid_ds[index]['answers'])\n",
        "\n",
        "  # for index in no_answer_indices:\n",
        "  # print(validation[index]['questions']['consensus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP5dKoKs5GgP"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  len_before = len(newsqa_valid_ds)\n",
        "  newsqa_valid_ds = removeNoAnswerIndices(newsqa_valid_ds, no_answer_indices)\n",
        "  len_after = len(newsqa_valid_ds)\n",
        "  print(f'Removed {len_before - len_after}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Y-Vd4qQnwI"
      },
      "source": [
        "Simple statistics number of answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdty9QOyRCYy"
      },
      "outputs": [],
      "source": [
        "# newsqa_ds = datasets.concatenate_datasets([newsqa_train_ds, newsqa_valid_ds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv-1tqVNQnwJ"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# answerLengthsBefore = list()\n",
        "# answerLengthsAfter = list()\n",
        "# for example in newsqa_ds:\n",
        "#   answerLengthsBefore.append(len(example['answers']))\n",
        "#   answerLengthsAfter.append(len(set(example['answers'])))\n",
        "\n",
        "# ds = list()\n",
        "# ds.append(answerLengthsBefore)\n",
        "# ds.append(answerLengthsAfter)\n",
        "# df_describe = pd.DataFrame(ds)\n",
        "# df_describe = df_describe.transpose()\n",
        "# df_describe.columns = ['answerLengthsBefore', 'answerLengthsAfter']\n",
        "# print(df_describe.describe())\n",
        "# print(sum(answerLengthsBefore) / len(answerLengthsBefore))\n",
        "# print(sum(answerLengthsAfter) / len(answerLengthsAfter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzA5FRlMR8FK"
      },
      "outputs": [],
      "source": [
        "# def autopct_format(values):\n",
        "#         def my_format(pct):\n",
        "#             total = sum(values)\n",
        "#             val = int(round(pct*total/100.0))\n",
        "#             return '{:.1f}%\\n({v:d})'.format(pct, v=val)\n",
        "#         return my_format\n",
        "\n",
        "# s = df_describe['answerLengthsAfter'].value_counts().sort_values(ascending = False)\n",
        "# top_k = 6\n",
        "# others = s[top_k:].sum()\n",
        "# print(s[top_k:].keys())\n",
        "# s = s.drop(s[top_k:].keys())\n",
        "# s['Others'] = others\n",
        "# print(s)\n",
        "# s.plot.pie(autopct=autopct_format(s),figsize=(5, 5))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hGf65jHq9R6E"
      },
      "source": [
        "#### SQuAD Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wP4-dWAkgNjq"
      },
      "source": [
        "Load squad dataset from \"datasets\" huggingface library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIGKkrt_Gbtd"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  squad_train_ds = load_dataset('squad', split='train')\n",
        "  squad_valid_ds = load_dataset('squad', split='validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6tbXoNmEHiU"
      },
      "outputs": [],
      "source": [
        "squad_ds = datasets.concatenate_datasets([squad_train_ds, squad_valid_ds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp8bBZoKL1nT"
      },
      "outputs": [],
      "source": [
        "len(squad_ds)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WhfJUoy8ESDX"
      },
      "source": [
        "Mapping every context to its answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DRT_DtnEPLa"
      },
      "outputs": [],
      "source": [
        "def map_context_to_answers(example, context_answers):\n",
        "  context = example['context']\n",
        "  answers = example['answers']['text']\n",
        "  current_context_answers = context_answers.get(context, set())\n",
        "  current_context_answers.update(answers)\n",
        "  context_answers[context] = current_context_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEiUI6VIEXUO"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "context_answers_squad = {}\n",
        "for example in tqdm(squad_ds):\n",
        "  map_context_to_answers(example,context_answers_squad)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H7nNM4btEeO_"
      },
      "source": [
        "Converting dictionary back to Huggingface dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ztBmJwfEk9n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "squad_ds = datasets.Dataset.from_pandas(pd.DataFrame(context_answers_squad.items(), columns=['context', 'answers']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cao0nbI42V1u"
      },
      "outputs": [],
      "source": [
        "print(len(squad_ds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WpQ3BQbUGiMz"
      },
      "source": [
        "Simple statistics number of answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efTH60hNDIp8"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# answerLengthsBefore = list()\n",
        "# answerLengthsAfter = list()\n",
        "# for example in squad_ds:\n",
        "#   answerLengthsBefore.append(len(example['answers']))\n",
        "#   answerLengthsAfter.append(len(set(example['answers'])))\n",
        "\n",
        "# ds = list()\n",
        "# ds.append(answerLengthsBefore)\n",
        "# ds.append(answerLengthsAfter)\n",
        "# df_describe = pd.DataFrame(ds)\n",
        "# df_describe = df_describe.transpose()\n",
        "# df_describe.columns = ['answerLengthsBefore', 'answerLengthsAfter']\n",
        "# print(df_describe.describe())\n",
        "# print(sum(answerLengthsBefore) / len(answerLengthsBefore))\n",
        "# print(sum(answerLengthsAfter) / len(answerLengthsAfter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHnK2j1pURDJ"
      },
      "outputs": [],
      "source": [
        "# def autopct_format(values):\n",
        "#         def my_format(pct):\n",
        "#             total = sum(values)\n",
        "#             val = int(round(pct*total/100.0))\n",
        "#             return '{:.1f}%\\n({v:d})'.format(pct, v=val)\n",
        "#         return my_format\n",
        "\n",
        "# s = df_describe['answerLengthsAfter'].value_counts().sort_values(ascending = False)\n",
        "# top_k = 6\n",
        "# others = s[top_k:].sum()\n",
        "# print(s[top_k:].keys())\n",
        "# s = s.drop(s[top_k:].keys())\n",
        "# s['Others'] = others\n",
        "# print(s)\n",
        "# s.plot.pie(autopct=autopct_format(s),figsize=(5, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN3L2qv_FE9J"
      },
      "outputs": [],
      "source": [
        "squad_ds_splits = squad_ds.train_test_split(test_size=0.1)\n",
        "squad_train_ds = squad_ds_splits[\"train\"]\n",
        "squad_valid_ds = squad_ds_splits[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBsK-ZXhMHgV"
      },
      "outputs": [],
      "source": [
        "print(type(squad_train_ds))\n",
        "print(len(squad_train_ds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TQE4OhrT9iZd"
      },
      "source": [
        "#### Encoding Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ta23fgnJb5w"
      },
      "source": [
        "Encoding examples (putting data in proper format for model & tokenizing the data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fAJO9uHNKL5"
      },
      "outputs": [],
      "source": [
        "def encode(example, encoder_max_len=encoder_max_len, decoder_max_len=decoder_max_len):\n",
        "    context = example['context']\n",
        "    answers=example['answers']\n",
        "    answers = ' EOA '.join([i for i in list(answers)]) # Comma seperated answers\n",
        "\n",
        "    input = task_prefix + 'context: ' + context\n",
        "    output = answers\n",
        "\n",
        "    encoder_inputs = tokenizer(input, truncation=True,\n",
        "                               return_tensors='tf', max_length=encoder_max_len,\n",
        "                              pad_to_max_length=True)\n",
        "    decoder_inputs = tokenizer(output, truncation=True,\n",
        "                               return_tensors='tf', max_length=decoder_max_len,\n",
        "                              pad_to_max_length=True)\n",
        "\n",
        "    # Shapes come from the encoder_max_len and decoder_max_len in hyperparameters section\n",
        "    input_ids = encoder_inputs['input_ids'][0] # Shape before flattening: input_ids.shape= (1, 250) [[1,1,3,...]]\n",
        "    input_attention = encoder_inputs['attention_mask'][0] # Shape before flattening: attension_mask.shape= (1, 250)\n",
        "    target_ids = decoder_inputs['input_ids'][0] # Shape before flattening: target_ids.shape= (1, 70)\n",
        "    target_attention = decoder_inputs['attention_mask'][0] # Shape before flattening: target_attention.shape= (1, 70)\n",
        "\n",
        "    outputs = {'input_ids':input_ids, 'attention_mask': input_attention,\n",
        "               'labels':target_ids, 'decoder_attention_mask':target_attention}\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYb7Tgp1Y_Ht"
      },
      "outputs": [],
      "source": [
        "if not is_ds_saved:\n",
        "  squad_train_ds = squad_train_ds.map(encode)\n",
        "  squad_valid_ds = squad_valid_ds.map(encode)\n",
        "  newsqa_train_ds = newsqa_train_ds.map(encode)\n",
        "  newsqa_valid_ds = newsqa_valid_ds.map(encode)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "72nbZexdoQoE"
      },
      "source": [
        "#### Save encoded datasets to Drive (requires mounting Drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak2i7wuYgttD"
      },
      "outputs": [],
      "source": [
        "save_folder_path = \"/content/drive/MyDrive/Datasets/AnswerExtraction\"\n",
        "\n",
        "if not is_ds_saved:\n",
        "  squad_train_ds.to_json(save_folder_path + \"/\" + \"squad-train-encoded-new.json\")\n",
        "  squad_valid_ds.to_json(save_folder_path + \"/\" + \"squad-valid-encoded-new.json\")\n",
        "  newsqa_train_ds.to_json(save_folder_path + \"/\" + \"newsqa-train-encoded-new.json\")\n",
        "  newsqa_valid_ds.to_json(save_folder_path + \"/\" + \"newsqa-valid-encoded-new.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh67aluEosGf"
      },
      "source": [
        "### Case 2: Datasets saved on Drive\n",
        "-----\n",
        "**Loading preprocessed and encoded datasets**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NuGpkYszobYa"
      },
      "source": [
        "**Load datasets from Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXtO6B0JIm4y"
      },
      "outputs": [],
      "source": [
        "save_folder_path = \"/content/drive/MyDrive/Datasets/AnswerExtraction\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M_jE8tPGH0W-"
      },
      "source": [
        "SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LElS3UKNhChx"
      },
      "outputs": [],
      "source": [
        "if is_ds_saved:\n",
        "  squad_data_files = {\n",
        "        \"train\": save_folder_path + \"/\" + \"squad-train-encoded.json\",\n",
        "        \"validation\": save_folder_path + \"/\" + \"squad-valid-encoded.json\",\n",
        "    }\n",
        "  squad_train_ds = load_dataset(\"json\", data_files=squad_data_files, split='train')\n",
        "  squad_valid_ds = load_dataset(\"json\", data_files=squad_data_files, split='validation')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J2_SukfBH1__"
      },
      "source": [
        "NewsQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFnsVLZWHocN"
      },
      "outputs": [],
      "source": [
        "if is_ds_saved:\n",
        "  newsqa_data_files = {\n",
        "        \"train\": save_folder_path + \"/\" + \"newsqa-train-encoded.json\",\n",
        "        \"validation\": save_folder_path + \"/\" + \"newsqa-valid-encoded.json\",\n",
        "    }\n",
        "  newsqa_train_ds = load_dataset(\"json\", data_files=newsqa_data_files, split='train')\n",
        "  newsqa_valid_ds = load_dataset(\"json\", data_files=newsqa_data_files, split='validation')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_odnDEESIM0j"
      },
      "source": [
        "###Merge Datasets & Convert to TensorFlow PrefetchDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY4ggjVpFE9J"
      },
      "outputs": [],
      "source": [
        "train_ds = datasets.concatenate_datasets([squad_train_ds, newsqa_train_ds])\n",
        "valid_ds = datasets.concatenate_datasets([squad_valid_ds, newsqa_valid_ds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qcpb3U5QMO7"
      },
      "outputs": [],
      "source": [
        "print(len(train_ds))\n",
        "print(len(valid_ds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e3MuTemTBU2D"
      },
      "source": [
        "**Convert to TensorFlow PrefetchDataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUydnR3Ka-Yo"
      },
      "outputs": [],
      "source": [
        "def to_tf_dataset(dataset):\n",
        "  columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\n",
        "  dataset.set_format(type='tensorflow', columns=columns)\n",
        "  return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32,\n",
        "                'labels':tf.int32, 'decoder_attention_mask':tf.int32,  }\n",
        "  return_shapes = {'input_ids': tf.TensorShape([None]), 'attention_mask': tf.TensorShape([None]),\n",
        "                  'labels': tf.TensorShape([None]), 'decoder_attention_mask':tf.TensorShape([None])}\n",
        "  ds = tf.data.Dataset.from_generator(lambda : dataset, return_types, return_shapes)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c61QywAEb7-W"
      },
      "outputs": [],
      "source": [
        "tf_train_ds = to_tf_dataset(train_ds)\n",
        "tf_valid_ds = to_tf_dataset(valid_ds)\n",
        "#tf_test_ds=to_tf_dataset(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRh47x7N_gxN"
      },
      "outputs": [],
      "source": [
        "tf_train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkCoA-YcfdKj"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, cache_path=None, batch_size=4,\n",
        "                   buffer_size= 1000, shuffling=True):\n",
        "    if cache_path is not None:\n",
        "        dataset = dataset.cache(cache_path) # ZZZ\n",
        "    dataset = dataset.repeat()  # 86k epoch\n",
        "    if shuffling:\n",
        "        dataset = dataset.shuffle(buffer_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbjNnG1YC8tw"
      },
      "outputs": [],
      "source": [
        "tf_train_ds= create_dataset(tf_train_ds, batch_size=batch_size,\n",
        "                           shuffling=True, cache_path = None)\n",
        "tf_valid_ds = create_dataset(tf_valid_ds, batch_size=batch_size,\n",
        "                           shuffling=False, cache_path = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC3r_KJ0gnA0"
      },
      "outputs": [],
      "source": [
        "tf_train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQTOJ8OKLv56"
      },
      "outputs": [],
      "source": [
        "# del train_ds, valid_ds, squad_train_ds, squad_valid_ds, newsqa_train_ds, newsqa_valid_ds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "27L6NxEKYWD_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1KpmbofTua93"
      },
      "source": [
        "Saving lowest val_loss model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTzUdYG4jNpA"
      },
      "outputs": [],
      "source": [
        "model = TFT5ForConditionalGeneration.from_pretrained(model_name) #options: t5-small, t5-base, t5-large, t5-3b, t5-11b\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CAPxr5fY3Lx"
      },
      "outputs": [],
      "source": [
        "epochs_done = 0\n",
        "total_num_of_epochs = 1\n",
        "ntrain = len(train_ds)\n",
        "nvalid = len(valid_ds)\n",
        "steps = ntrain // batch_size\n",
        "valid_steps = nvalid // batch_size\n",
        "print(\"Total Steps: \", steps)\n",
        "print(\"Total Validation Steps: \", valid_steps)\n",
        "model.fit(tf_train_ds, epochs=total_num_of_epochs, steps_per_epoch=steps, validation_data=tf_valid_ds, validation_steps=valid_steps, initial_epoch=epochs_done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpoih_fPZtKX"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(f'{model_name}-epochs={total_num_of_epochs}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuiXUo_qojAT"
      },
      "outputs": [],
      "source": [
        "saved_model_dir_src = f'{model_name}-epochs={total_num_of_epochs}'\n",
        "saved_model_dir_dest = \"/content/drive/MyDrive/AnswerExtractionModels\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oPOWS_8nMBB"
      },
      "outputs": [],
      "source": [
        "!cp -r {saved_model_dir_src} {saved_model_dir_dest}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "13QCfbF_ehYe"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OfF12w8nfAVh"
      },
      "source": [
        "## Manual"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNnbqvzityI"
      },
      "source": [
        "**Single Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUMDD2jmgl3U"
      },
      "outputs": [],
      "source": [
        "context = \"In a broad sense, all of computer security is concerned with access control. Indeed, RFC 4949 defines computer security as follows: measures that implement and assure security services in a computer system, particularly those that assure access control service. This chapter deals with a narrower, more specific concept of access control: Access control implements a security policy that specifies who or what (e.g., in the case of a process) may have access to each specific system resource, and the type of access that is permitted in each instance.\" #@param {type:\"string\"}\n",
        "context = context.strip()\n",
        "input_text = task_prefix + 'context: ' + context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v5T5uE8MC92"
      },
      "outputs": [],
      "source": [
        "encoded_query = tokenizer(input_text, return_tensors='tf', pad_to_max_length=True, truncation=True, max_length=encoder_max_len)\n",
        "\n",
        "input_ids = encoded_query[\"input_ids\"]\n",
        "attention_mask = encoded_query[\"attention_mask\"]\n",
        "generated_answers = model.generate(input_ids, attention_mask=attention_mask, max_length=decoder_max_len, top_p=0.95, top_k=50, repetition_penalty=float(2))\n",
        "decoded_answers = tokenizer.decode(generated_answers.numpy()[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Answers: \", decoded_answers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YU3jeDq-ixwt"
      },
      "source": [
        "**Multiple Examples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLzA_Iocj57X"
      },
      "outputs": [],
      "source": [
        "# use different length sentences to test batching\n",
        "\n",
        "sentences = [\n",
        "  \"In a broad sense, all of computer security is concerned with access control. Indeed, RFC 4949 defines computer security as follows: measures that implement and assure security services in a computer system, particularly those that assure access control service. This chapter deals with a narrower, more specific concept of access control: Access control implements a security policy that specifies who or what (e.g., in the case of a process) may have access to each specific system resource, and the type of access that is permitted in each instance.\"\n",
        "  , \"A subject is an entity capable of accessing objects. Generally, the concept of subject equates with that of process. Any user or application actually gains access to an object by means of a process that represents that user or application. The process takes on the attributes of the user, such as access rights.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSJk6J2QMC93"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer([task_prefix + 'context: ' + sentence for sentence in sentences], return_tensors=\"tf\", padding=True)\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        ")\n",
        "\n",
        "tokenizer.batch_decode(output_sequences, skip_special_tokens=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "duFfYl-efCdJ"
      },
      "source": [
        "## Automatic"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n0xJ2azcA4Fd"
      },
      "source": [
        "**Inference on training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHQT5HYnAS8m"
      },
      "outputs": [],
      "source": [
        "extracted_answers = list()\n",
        "for i in tqdm(range(0, len(train_ds['input_ids']), 700)):\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=train_ds[\"input_ids\"][i:i+700],\n",
        "      attention_mask=train_ds[\"attention_mask\"][i:i+700],\n",
        "      max_length=decoder_max_len,\n",
        "      top_p=0.95,\n",
        "      top_k=50,\n",
        "      repetition_penalty=float(2)\n",
        "  )\n",
        "  a = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
        "  extracted_answers.extend(a)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MJCtVV6FLHRf"
      },
      "source": [
        "**Saving inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LecqoEUKKuUq"
      },
      "outputs": [],
      "source": [
        "print(len(extracted_answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XAh4gt4LcRF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "size_in_mb = os.path.getsize(\"/content/extracted_answers.pickle\") / 10**6\n",
        "print(f'extracted_answers.pickle size: {size_in_mb}MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4-8nXz2LHRg"
      },
      "outputs": [],
      "source": [
        "with open('extracted_answers.pickle', 'wb') as f:\n",
        "    pickle.dump(extracted_answers, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW4NJeUM0hg-"
      },
      "outputs": [],
      "source": [
        "!cp \"extracted_answers.pickle\" \"/content/drive/MyDrive/AnswerExtractionModels\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dV71B0OR3bFO"
      },
      "source": [
        "**Loading inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3WISO6K1VQw"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/AnswerExtractionModels/extracted_answers.pickle\" \"extracted_answers.pickle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDi2ZEFq1Pvr"
      },
      "outputs": [],
      "source": [
        "with open('extracted_answers.pickle', 'rb') as f:\n",
        "    extracted_answers = pickle.load(f)\n",
        "\n",
        "print('extracted_answers is', len(extracted_answers))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1-kdDsuh5UGW"
      },
      "source": [
        "**Transform list of extracted answers for each context to list of list**\n",
        "\n",
        "**Ex:**\n",
        "```\n",
        "myList = ['car, boat, vehicle']\n",
        "myListTransformed = [['car', 'boat', 'vehicle']]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXdP3IKc6mSg"
      },
      "outputs": [],
      "source": [
        "extracted_answers = [answersString.split(',') for answersString in extracted_answers]\n",
        "print(extracted_answers[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tNYS71Uk3j3s"
      },
      "source": [
        "**Calculating F1 score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-szu6KE_-b6n"
      },
      "outputs": [],
      "source": [
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "995o96sD-Xob"
      },
      "outputs": [],
      "source": [
        "def compute_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return tuple([int(gold_toks == pred_toks)] * 3)\n",
        "  if num_same == 0:\n",
        "    return tuple([0] * 3)\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return (f1, precision, recall)\n",
        "\n",
        "def compute_f1_avg(f1_scores, total):\n",
        "  return (sum(f1_scores) / total) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI3KdpHpMN5E"
      },
      "outputs": [],
      "source": [
        "def compute_em(a_gold, a_pred):\n",
        "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "def compute_em_avg(em_scores, total):\n",
        "  return (sum(em_scores) / total) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwVSU2ZC1ZxU"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "f1_scores = list()\n",
        "precison_scores = list()\n",
        "recall_scores = list()\n",
        "em_scores = list()\n",
        "\n",
        "for ground_truth_answers_list, extracted_answers_list in tqdm(list(zip(train_ds['answers'], extracted_answers))):\n",
        "  for extracted_answer in extracted_answers_list:\n",
        "    # ***IMPORTANT***\n",
        "    # - Problem: Model outputs empty string for no answers sometimes\n",
        "    # - Temporary fix by setting the conditon for default value to check if the\n",
        "    #   current generated answer is an empty string (no answer)\n",
        "    #   and setting the result to 1 as that means both ground truth and\n",
        "    #   generated answer agree that there is no answers to extract\n",
        "    # - Actual fix by removing the empty answers and retraining the model\n",
        "\n",
        "    result = [compute_f1(ground_truth_answer, extracted_answer) for ground_truth_answer in ground_truth_answers_list]\n",
        "    f1_score, precision, recall = max(result, key=itemgetter(0)) if(len(result) != 0) else tuple([int(extracted_answer == '')]*3)\n",
        "    # f1_score = max([compute_f1(ground_truth_answer, extracted_answer) for ground_truth_answer in ground_truth_answers_list], default= int(extracted_answer == ''))\n",
        "    em_score = max([compute_em(ground_truth_answer, extracted_answer) for ground_truth_answer in ground_truth_answers_list], default= int(extracted_answer == ''))\n",
        "    f1_scores.append(f1_score)\n",
        "    precison_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    em_scores.append(em_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fjHhPAHRLue"
      },
      "outputs": [],
      "source": [
        "print(len(f1_scores))\n",
        "print(len(em_scores))\n",
        "\n",
        "print(list(zip(f1_scores[:100], em_scores[:100])))\n",
        "\n",
        "print('F1 =', compute_f1_avg(f1_scores, len(f1_scores)), '%')\n",
        "print('EM =', compute_em_avg(em_scores, len(em_scores)), '%')\n",
        "print('Precision =', compute_f1_avg(precison_scores, len(f1_scores)), '%')\n",
        "print('Recall =', compute_f1_avg(recall_scores, len(f1_scores)), '%')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RYWVQ51kULrz"
      },
      "source": [
        "Scores before adding conditon in default: different in the 3rd digit in the precentage\n",
        "\n",
        "F1 = 46.97654860625854 %\n",
        "\n",
        "EM = 32.95584116546556 %"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Xf9plwkRCtJF",
        "1lkEgQ2qGLYD",
        "9d0G5gXZEzv0",
        "5UgN1eJOFf4F",
        "GM1X7HIdgAXs",
        "xEic2hRcoYAS",
        "hGf65jHq9R6E",
        "Wh67aluEosGf",
        "_odnDEESIM0j",
        "27L6NxEKYWD_",
        "13QCfbF_ehYe",
        "OfF12w8nfAVh",
        "duFfYl-efCdJ",
        "nG5KPqXKCrAx",
        "KFrun5Z9DZzr",
        "kZn6hsvYDYnK",
        "AouT9Ta_Deqm",
        "Vwg4E1EgDqMD",
        "i0jw9AFnvgsC",
        "2pF6WuQzVuRt",
        "jIOXRDJR9YR0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
