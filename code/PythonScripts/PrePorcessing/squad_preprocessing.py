# -*- coding: utf-8 -*-
"""squad-preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12wUiVNbjZcNPxcxaWwWdpi-wFKKXXlI0

# Install Libraries
"""

!pip install transformers
!pip install datasets

"""## Packages"""

import torch
import transformers
import spacy
import numpy as np
import json,time
from pathlib import Path
from tqdm import tqdm
import locale
import nltk
nltk.download('punkt')
nlp = spacy.load('en_core_web_sm')

tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')

from google.colab import drive
drive.mount('/content/drive')

train_ds_file = Path(r'/content/drive/MyDrive/Datasets/train_squad_dataset.pt')
val_ds_file = Path(r'/content/drive/MyDrive/Datasets/val_squad_dataset.pt')

is_train_ds_saved = False
is_val_ds_saved = False

if train_ds_file.is_file():
    is_train_ds_saved = True

if val_ds_file.is_file():
    is_val_ds_saved = True

"""# Loading SquadV2 Dataset"""

is_train_ds_saved = False
if not is_train_ds_saved:
  def getpreferredencoding(do_setlocale = True):
      return "UTF-8"
  locale.getpreferredencoding = getpreferredencoding

  # Load SQuAD v2 dataset and preprocess data

  !mkdir squadV2_ds
  !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squadV2_ds/train-v2.0.json
  !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squadV2_ds/dev-v2.0.json

"""# Retrieve and Store the data"""

# Give the path for train data
if not is_train_ds_saved:
  path = Path('/content/squadV2_ds/train-v2.0.json')

  # Open .json file
  with open(path, 'rb') as f:
      squad_dict = json.load(f)

texts = []
queries = []
answers = []

# Search for each passage, its question and its answer
for group in squad_dict['data']:
    for passage in group['paragraphs']:
        context = passage['context']
        for qa in passage['qas']:
            question = qa['question']
            for answer in qa['answers']:
                # Store every passage, query and its answer to the lists
                texts.append(context)
                queries.append(question)
                answers.append(answer['text'])

passages_train, questions_train, answers_train = texts, queries, answers

# Give the path for train data
if not is_train_ds_saved:
  path = Path('/content/squadV2_ds/dev-v2.0.json')

  # Open .json file
  with open(path, 'rb') as f:
      squad_dict = json.load(f)

  texts = []
  queries = []
  answers = []

  # Search for each passage, its question and its answer
  for group in squad_dict['data']:
      for passage in group['paragraphs']:
          context = passage['context']
          for qa in passage['qas']:
              question = qa['question']
              for answer in qa['answers']:
                  # Store every passage, query and its answer to the lists
                  texts.append(context)
                  queries.append(question)
                  answers.append(answer['text'])

  passages_val, questions_val, answers_val = texts, queries, answers

# Initialize a dictionary to store the counts
# List of interrogative words
interrogative_words = ["who", "what", "where", "when", "why", "how","which"]
word_counts = {word: 0 for word in interrogative_words}
# Iterate over each question
for question in questions_train:
    # Tokenize the question into words
    words = nltk.word_tokenize(question)

    # Convert words to lowercase
    words = [word.lower() for word in words]

    # Count the interrogative words
    for word in words:
        if word in interrogative_words:
            word_counts[word] += 1

# Print the word counts
for word, count in word_counts.items():
    print(word + ": " + str(count))

answers = list()
questions = list()
passages = list()

word_counts = {word: 0 for word in interrogative_words}

word_counts['what'] +=6000
word_counts['who'] +=6000
word_counts['when'] +=6000
word_counts['how'] +=6000
word_counts['which'] +=6000
word_counts['where'] +=3884
word_counts['why'] +=1277

for idx,question in enumerate(questions_train):
    words = nltk.word_tokenize(question)

    # Convert words to lowercase
    words = [word.lower() for word in words]

    # Count the interrogative words
    for word in words:
      if word in interrogative_words and word_counts[word]!=0:
        answers.append(answers_train[idx])
        questions.append(question)
        passages.append(passages_train[idx])
        word_counts[word] -=1

print(len(questions))
print(len(answers))
print(len(passages))

for word, count in word_counts.items():
    print(word + ": " + str(count))

"""# Tokenization






"""

# training data tokenization
if not is_train_ds_saved:
  input_ids_train = []
  attention_masks_train = []

  # iterates through pairs of passages and answers and calls tokenizer.encode_plus() on each pair.
  # tokenizes a piece of text(passages, answers) and returns a dictionary containing various pieces of information about the tokens.
  '''
    the resulting dictionary contains two keys: input_ids and attention_mask.
    input_ids is a tensor that contains the token IDs of the input sequence,
    and attention_mask is a tensor that contains a binary mask indicating which tokens are padding and which ones are not.
  '''
  for passage, answer in tqdm(zip(passages, answers)):
      encoded_dict = tokenizer.encode_plus(
                          answer,
                          passage,
                          add_special_tokens = True,
                          max_length = 512,
                          pad_to_max_length = True,
                          return_attention_mask = True,
                          return_tensors = 'pt'
                    )
      input_ids_train.append(encoded_dict['input_ids'])
      attention_masks_train.append(encoded_dict['attention_mask'])

# validation data tokenization
if not is_train_ds_saved:
  input_ids_val = []
  attention_masks_val = []

  for passage, answer in zip(passages_val, answers_val):
      encoded_dict = tokenizer.encode_plus(
                          answer,
                          passage,
                          add_special_tokens = True,
                          max_length = 512,
                          pad_to_max_length = True,
                          return_attention_mask = True,
                          return_tensors = 'pt'
                    )
      input_ids_val.append(encoded_dict['input_ids'])
      attention_masks_val.append(encoded_dict['attention_mask'])

"""# Entity Name Recognition"""

# dictionary to map each entity to its tensor
entity_type_dict = {'PERSON': [1, 0, 0, 0, 0], 'CARDINAL': [0, 1, 0, 0, 0], 'DATE': [0, 0, 1, 0, 0], 'ORG': [0, 0, 0, 1, 0], 'GPE': [0, 0, 0, 0, 1], 'None': [0, 0, 0, 0, 0]}
interrogative_words = {'who':0, 'what':1, 'when':2, 'where':3, 'why':4, 'how':5,'which':6,'other':7}

# Obtain entity type of answer using spaCy and create learnable embedding
if not is_train_ds_saved:
entity_type_embeddings = list()
for answer in tqdm(answers):
    doc = nlp(answer)
    if doc.ents:
      entity_types = set([token.ent_type_ if token.ent_type_ != "" else 'None' for token in doc])
    else:
      entity_types = set(['None'])
    entities = set()
    entity_list = list()
    for ent in entity_types:
      if ent in entity_type_dict:
        entities.add(ent)
      else :
        entities.add('None')

    for ent in entities:
      try:
        entity = (entity_type_dict[ent])
      except:
        entity = (entity_type_dict['None'])
      entity_list.append(entity)

    enity_length = len(entity_list)
    if enity_length<6:
      for i in range(6-enity_length):
        entity_list.append(entity_type_dict['None'])

    entity_type_embeddings.append(entity_list)

entity_type_embeddings_train = entity_type_embeddings

# Obtain entity type of answer using spaCy and create learnable embedding
if not is_train_ds_saved:
  entity_type_embeddings_val = list()
for answer in tqdm(answers_val):
    doc = nlp(answer)
    if doc.ents:
      entity_types = set([token.ent_type_ if token.ent_type_ != "" else 'None' for token in doc])
    else:
      entity_types = set(['None'])
    entities = set()
    entity_list = list()
    for ent in entity_types:
      if ent in entity_type_dict:
        entities.add(ent)
      else :
        entities.add('None')

    for ent in entities:
      try:
        entity = (entity_type_dict[ent])
      except:
        entity = (entity_type_dict['None'])
      entity_list.append(entity)

    enity_length = len(entity_list)
    if enity_length<6:
      for i in range(6-enity_length):
        entity_list.append(entity_type_dict['None'])

    entity_type_embeddings_val.append(entity_list)

"""# Extract Question Word"""

# dictionary to map each token
if not is_train_ds_saved:
  interrogative_words = {'who':0, 'what':1, 'when':2, 'where':3, 'why':4, 'how':5,'which':6,'other':7}

  found_interrogative_words_train = []
  for q in tqdm(questions):
    words = tokenizer.tokenize(q)
    for word in words:
      found=False
      if word.lower() in interrogative_words:
          found=True
          found_interrogative_words_train.append(interrogative_words[word.lower()])
          break
    if found==False:
          found_interrogative_words_train.append(interrogative_words['other'])

# dictionary to map each token
if not is_train_ds_saved:
  interrogative_words = {'who':0, 'what':1, 'when':2, 'where':3, 'why':4, 'how':5,'which':6,'other':7}
  found_interrogative_words_val = []
  for q in tqdm(questions_val):
    words = tokenizer.tokenize(q)
    for word in words:
      found=False
      if word.lower() in interrogative_words:
          found=True
          found_interrogative_words_val.append(interrogative_words[word.lower()])
          break
    if found==False:
          found_interrogative_words_val.append(interrogative_words['other'])

"""# Loading Data to TensorDataset"""

# Convert data to tensors
if not is_train_ds_saved:
  input_ids_train = torch.cat(input_ids_train, dim=0)
  attention_masks_train = torch.cat(attention_masks_train, dim=0)
  entity_type_embeddings_train = torch.tensor(entity_type_embeddings_train)
  interrogative_words_labels_train=torch.tensor(found_interrogative_words_train,dtype=torch.int64)

if is_val_ds_saved ==False:
  dataset_train = torch.utils.data.TensorDataset(input_ids_train, attention_masks_train, entity_type_embeddings_train,interrogative_words_labels_train)
  torch.save(dataset_train,train_ds_file)


if is_val_ds_saved ==False:
  dataset_val = torch.utils.data.TensorDataset(input_ids_val, attention_masks_val, entity_type_embeddings_val,interrogative_words_labels_val)
  torch.save(dataset_val,val_ds_file)