# -*- coding: utf-8 -*-
"""newsqa-preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ce1WXmYi85o-KRu5Mi9K2KGYnhJ9ySU6

# Instal Packages
"""

!pip install transformers
!pip install datasets

import datasets
import transformers
import datetime
import os
from datasets import load_dataset
from datasets import Features, Value, Sequence
from tqdm import tqdm
from string import punctuation
import spacy
import numpy as np
from pathlib import Path
import torch
import locale
import spacy.cli
# spacy.cli.download("en_core_web_lg")
nlp = spacy.load('en_core_web_sm')
import nltk
nltk.download('punkt')

"""# Load NewsQA Dataset"""

def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding


!gdown --folder 1ujEc3UsU73RakkZ9RSYnlfbTyIcyRUAm

newsqa_dataset_dir = "/content/NewsQaDataset"

train = load_dataset("newsqa", split="train",data_dir=newsqa_dataset_dir,name="combined-json")

print(next(iter(train)))
print(len(train))

"""# Preprocessing Dataset"""

'''
  Remove Unnecessery columns
'''

train=train.remove_columns(['storyId','type'])
temp = train.flatten()
train=temp.remove_columns(['questions.answers', 'questions.validated_answers','questions.isAnswerAbsent','questions.isQuestionBad'])
train=train.rename_column("text", "context")
train=train.rename_column("questions.consensus", "answers")

# punctuation = !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~

'''
  Extract Answers from

'''
def getAnswers_QuestionsStrings(example):
  answer_list = []
  question_list = []
  for idx,answer in enumerate(example['answers']):
    if answer['noAnswer']==True or answer['badQuestion'] == True:
      continue
    answerstring=example['context'][answer['s']:answer['e']]
    answerstring=answerstring.strip()
    answerstring=answerstring.strip(punctuation)
    answer_list.append(answerstring)
    question_list.append(example['questions.q'][idx])

  example['questions.q'] = question_list
  example['answers'] = answer_list
  return example

newsqa = train.map(getAnswers_QuestionsStrings, num_proc=16)

print(next(iter(newsqa)))

def extraction(dataset):
  passages = []
  queries  = []
  answers  = []
  for group in (dataset):
    passages.append(group['context'])
    queries.append(group['questions.q'])
    answers.append(group['answers'])

  return passages, queries, answers

passages, questions, answers = extraction(newsqa)

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

"""## Mapping Answers and Question To Right Context"""

passage_train = []
for idx,passage in enumerate(passages):
    for i in range(len(questions[idx])):
      passage_train.append(passage)

answer_train = []


for answer in (answers):
  for ans in answer:
    answer_train.append(ans)

question_train = []

for question in (questions):
  for q in question:
    question_train.append(q)

answers_ = list()
questions_ = list()
passages_ = list()
pp = set()
for idx,answer in enumerate(answer_train):
  if answer in passage_train[idx]:
    words = passage_train[idx].split()
    start_index = passage_train[idx].lower().find(answer.lower())
    last_index = start_index + len(answer)
    if start_index < 2500 and last_index<=2500:
        passages_.append(" ".join(words[:550]))
        answers_.append(answer)
        questions_.append(question_train[idx])

    pp.add(len(passages_[-1]))

# Initialize a dictionary to store the counts
# List of interrogative words
interrogative_words = ["who", "what", "where", "when", "why", "how","which"]
word_counts = {word: 0 for word in interrogative_words}
# Iterate over each question
for question in questions_:
    # Tokenize the question into words
    words = nltk.word_tokenize(question)

    # Convert words to lowercase
    words = [word.lower() for word in words]

    # Count the interrogative words
    for word in words:
        if word in interrogative_words:
            word_counts[word] += 1

# Print the word counts
for word, count in word_counts.items():
    print(word + ": " + str(count))

answers = list()
questions = list()
passages = list()
pp = set()
word_counts = {word: 0 for word in interrogative_words}

word_counts['what'] +=2000
word_counts['who'] +=2000
word_counts['when'] +=4000
word_counts['how'] +=2000
word_counts['which'] +=2000
word_counts['where'] +=4621
word_counts['why'] +=131

for idx,question in enumerate(questions_):
    words = nltk.word_tokenize(question)

    # Convert words to lowercase
    words = [word.lower() for word in words]

    # Count the interrogative words
    for word in words:
      if word in interrogative_words and word_counts[word]!=0 :
        answers.append(answers_[idx])
        questions.append(question)
        passages.append(passages_[idx])
        pp.add(len(passages_[idx]))
        word_counts[word] -=1

print(len(questions))
print(len(answers))
print(len(passages))

print((questions[323]))
print((answers[323]))
print((passages[323]))

"""# Passages, Answers pairs Tokanization"""

# iterates through pairs of passages and answers and calls tokenizer.encode_plus() on each pair.
# tokenizes a piece of text(passages, answers) and returns a dictionary containing various pieces of information about the tokens.
'''
  the resulting dictionary contains two keys: input_ids and attention_mask.
  input_ids is a tensor that contains the token IDs of the input sequence,
  and attention_mask is a tensor that contains a binary mask indicating which tokens are padding and which ones are not.
'''


def tokenize_PassageAnswer_pairs(passages, answers):
  input_ids = []
  attention_masks = []

  for passage, answer in tqdm(zip(passages, answers)):
      encoded_dict = tokenizer.encode_plus(
                          passage,
                          answer,
                          add_special_tokens = True,
                          return_token_type_ids = False,
                          max_length = 512,
                          truncation=True,
                          truncation_strategy='only_first',
                          padding="max_length",
                          return_attention_mask = True,
                          return_tensors = 'pt'
                    )
      input_ids.append(encoded_dict['input_ids'][:,:512])
      attention_masks.append(encoded_dict['attention_mask'])
  return input_ids,attention_masks

input_ids_train, attention_masks_train = tokenize_PassageAnswer_pairs(passages, answers)

len(input_ids_train)

"""# Helping Dictionaries"""

# dictionary to map each entity to its tensor
entity_type_dict = {'PERSON': [1, 0, 0, 0, 0], 'CARDINAL': [0, 1, 0, 0, 0], 'DATE': [0, 0, 1, 0, 0], 'ORG': [0, 0, 0, 1, 0], 'GPE': [0, 0, 0, 0, 1], 'None': [0, 0, 0, 0, 0]}
interrogative_words = {'who':0, 'what':1, 'when':2, 'where':3, 'why':4, 'how':5,'which':6,'other':7}

"""# Name Entity Recogniction

> Entity Type: PERSON, Count: 16510
> Entity Type: NORP, Count: 4424
> Entity Type: GPE, Count: 13727
> Entity Type: ORG, Count: 8432
> Entity Type: ORDINAL, Count: 808
> Entity Type: QUANTITY, Count: 821
> Entity Type: FAC, Count: 464
> Entity Type: TIME, Count: 815
> Entity Type: MONEY, Count: 881
> Entity Type: PERCENT, Count: 469
> Entity Type: PRODUCT, Count: 262
> Entity Type: LOC, Count: 1082
> Entity Type: WORK_OF_ART, Count: 399
> Entity Type: EVENT, Count: 304
> Entity Type: LANGUAGE, Count: 63
> Entity Type: LAW, Count: 66



"""

entity_type_embeddings = list()
for answer in tqdm(answers):
    doc = nlp(answer)
    if doc.ents:
      entity_types = set([token.ent_type_ if token.ent_type_ != "" else 'None' for token in doc])
    else:
      entity_types = set(['None'])
    entities = set()
    entity_list = list()
    for ent in entity_types:
      if ent in entity_type_dict:
        entities.add(ent)
      else :
        entities.add('None')

    for ent in entities:
      try:
        entity = (entity_type_dict[ent])
      except:
        entity = (entity_type_dict['None'])
      entity_list.append(entity)

    enity_length = len(entity_list)
    if enity_length<6:
      for i in range(6-enity_length):
        entity_list.append(entity_type_dict['None'])

    entity_type_embeddings.append(entity_list)

"""## Question Toknaization"""

def question_InterrogativeWord_Extraction(questions):
  found_interrogative_words = []
  for q in tqdm(questions):
    words = tokenizer.tokenize(q)
    for word in words:
      found = False
      if word.lower() in interrogative_words:
          found=True
          found_interrogative_words.append(interrogative_words[word.lower()])
          break
    if found==False:
          found_interrogative_words.append(interrogative_words['other'])

  return found_interrogative_words

found_interrogative_words_train = question_InterrogativeWord_Extraction(questions)

len(found_interrogative_words_train)

"""# Convert Arrays to Tensors"""

input_ids_train = torch.cat(input_ids_train, dim=0)
attention_masks_train = torch.cat(attention_masks_train, dim=0)
entity_type_embeddings_train = torch.tensor(entity_type_embeddings)
interrogative_words_labels_train=torch.tensor(found_interrogative_words_train,dtype=torch.int64)

"""# Saving Preprocessed Dataset"""

from google.colab import drive
drive.mount('/content/drive')

train_ds_file = Path(r'/content/drive/MyDrive/Datasets/train_newsqa_dataset.pt')

print(input_ids_train.shape)
print(attention_masks_train.shape)
print(entity_type_embeddings_train.shape)
print(interrogative_words_labels_train.shape)

dataset_train = torch.utils.data.TensorDataset(input_ids_train, attention_masks_train, entity_type_embeddings_train,interrogative_words_labels_train)
torch.save(dataset_train,train_ds_file)