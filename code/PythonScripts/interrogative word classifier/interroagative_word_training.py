# -*- coding: utf-8 -*-
"""Interroagative_word-Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18AFhCciSLAT-yHXBYMj_TR8KJQ5dDaf_

**LAST UDPDATE 15/6 BY LABIB**

# install Packages
"""

!pip install transformers

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
import torch
import transformers
from pathlib import Path
from transformers import AdamW
from tqdm import tqdm
import time
import gc

"""# Cude Device"""

device = torch.device('cuda:0' if torch.cuda.is_available()
                      else 'cpu')

"""# Load Bert "base" & Tokenizer"""

# Load BERT-base-uncased model and tokenizer

model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=8).to(device)
tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')

"""# Load datasets"""

from google.colab import drive
drive.mount('/content/drive')

train_adversarial_qa_ds_file = Path(r'/content/drive/MyDrive/Datasets/train_adversarial_qa_dataset.pt')
train_NEWSQA_ds_file = Path(r'/content/drive/MyDrive/Datasets/train_newsqa_dataset.pt')
train_SQuAD_ds_file = Path(r'/content/drive/MyDrive/Datasets/train_squad_dataset.pt')

eval_SQuAD_ds_file = Path(r'/content/drive/MyDrive/Datasets/val_squad_dataset.pt')


dataset_SQuAD_val = torch.load(eval_SQuAD_ds_file)

dataset_adversarial_qa_train = torch.load(train_adversarial_qa_ds_file)
dataset_NEWSQA_train = torch.load(train_NEWSQA_ds_file)
dataset_SQuAD_train = torch.load(train_SQuAD_ds_file)

"""## Concatenate Datasets"""

input_ids ,attantion,entity,labels = dataset_SQuAD_train.tensors
input_ids1 , attantion1 , entity1 , labels1  = dataset_adversarial_qa_train.tensors
input_ids2 , attantion2 , entity2 , labels2  = dataset_NEWSQA_train.tensors

input_ids =  torch.cat([input_ids, input_ids1, input_ids2], dim=0)
attantion =  torch.cat([attantion, attantion1, attantion2], dim=0)
entity =  torch.cat([entity, entity1,entity2], dim=0)
labels =  torch.cat([labels, labels1, labels2], dim=0)

print(len(input_ids))
print(len(attantion))
print(len(entity))
print(len(labels))

train_dataset =  torch.utils.data.TensorDataset(input_ids,attantion,entity,labels)
dataloader_train = torch.utils.data.DataLoader(train_dataset, batch_size=16,shuffle=True, num_workers=8, pin_memory=True,sampler=None)
dataloader_eval = torch.utils.data.DataLoader(dataset_SQuAD_val, batch_size=16,shuffle=True, num_workers=8, pin_memory=True,sampler=None)

print(len(dataloader_train))
print(len(dataloader_eval))

mapping_vector = { 0 : 'who', 1 : 'what', 2 : 'when', 3 : 'where', 4 : 'why', 5 : 'how',6 : 'which',7 : 'other'}

"""#FeedForwardNN"""

class FFN(torch.nn.Module):
    def __init__(self, hidden_size, num_classes):
        super().__init__()
        self.fc1 = torch.nn.Linear(hidden_size, num_classes).to(device)
        self.softmax = torch.nn.Softmax(dim=-1).to(device)

    def forward(self, x):
        out = self.fc1(x)
        out = self.softmax(out)
        return out

"""# Hyperparameters"""

epochs = 2
ffn = FFN(hidden_size=773, num_classes=8)
softmax=torch.nn.Softmax(dim=-1)
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(ffn.parameters(), lr=5e-5, weight_decay=5e-5)
model_optimizer = AdamW(model.parameters(), lr=5e-5)

train_losses = []
train_model_losses = []
val_losses = []
val_model_losses = []

print_every = 1000

"""# Loading Model"""

model_path = Path(r'/content/drive/MyDrive/Datasets/training_base_model/old/checkpoints-base-2.pth')

checkpoint = torch.load(model_path,map_location=torch.device('cpu'))

model.load_state_dict(checkpoint['model_state_dict'])
model_optimizer.load_state_dict(checkpoint['model_optimizer_state_dict'])
ffn.load_state_dict(checkpoint['ffn_state_dict'])
optimizer.load_state_dict(checkpoint['ffn_optimizer_state_dict'])

model_path = Path(r'/content/drive/MyDrive/Datasets/training_base_model/training_26_6.pth')

best_acc = checkpoint['accuracy']
# best_acc = 0
best_acc

"""# Training"""

for epoch in tqdm(range(epochs)):

    epoch_time = time.time()
    epoch_acc=0
    loss_of_epoch_model = 0
    loss_of_epoch = 0
    num_correct = 0
    num_total = 0
    num_correct_train = 0
    num_total_train = 0

    # Set model in train mode
    model.train()
    print("\n############Train############")

    gc.collect()
    for batch_idx,batch in enumerate(dataloader_train):
        if (batch_idx+1) % print_every == 0:
          model_optimizer.zero_grad()

        input_ids_batch, attention_masks_batch, entity_type_embeddings_batch,labels= tuple(t.to(device) for t in batch)

        outputs = model(input_ids= input_ids_batch, attention_mask= attention_masks_batch, labels= labels,output_hidden_states=True)


        if (batch_idx+1) % print_every == 0:
          model_loss = outputs.loss

        cls_tensor = outputs.hidden_states[-1]
        cls_tensor = cls_tensor[:,:6,:]
        ner_tensor = entity_type_embeddings_batch #.to(torch.float32)
        # Concatenate the tensors along the first dimension
        concat_tensor = torch.cat((cls_tensor, ner_tensor),dim=2).to(device)

        logits = ffn(concat_tensor)
        logits=(torch.max(logits,dim=1).values)

        loss = loss_fn(logits, labels)

        optimizer.zero_grad()

        if (batch_idx+1) % print_every == 0:
          model_loss.backward(retain_graph=True)

        loss.backward()

        if (batch_idx+1) % print_every == 0:
          model_optimizer.step()

        optimizer.step()

        if (batch_idx+1) % print_every == 0:
          loss_of_epoch_model += model_loss.item()

        loss_of_epoch +=loss.item()

        predictions = torch.argmax(logits, dim=1)
        num_correct_train += (predictions == labels).sum().item()
        num_total_train += len(labels)

        if (batch_idx+1) % print_every == 0:
          print("Batch {:} / {:}".format(batch_idx+1,len(dataloader_train)),"\nLoss:", round(loss.item(),1))
          print("predictions : ",predictions,"\n","Labels : ",labels,"\n")

        if (batch_idx+1) % 100 == 0:
          torch.cuda.empty_cache()
          gc.collect()

    train_accuracy = num_correct_train / num_total_train
    loss_of_epoch /= len(dataloader_train)
    loss_of_epoch_model /= len(dataloader_train)
    train_losses.append(loss_of_epoch)
    train_model_losses.append(loss_of_epoch_model)


    model.eval()

    print("\n############Evaluate############")

    loss_of_epoch = 0
    loss_of_epoch_model = 0
    gc.collect()
    for batch_idx,batch in enumerate(dataloader_eval):

        input_ids_batch, attention_masks_batch, entity_type_embeddings_batch,labels= tuple(t.to(device) for t in batch)

        with torch.no_grad():
          outputs = model(input_ids_batch, attention_mask=attention_masks_batch,output_hidden_states=True)

        cls_tensor = outputs.hidden_states[-1]
        cls_tensor = cls_tensor[:,:6,:]
        ner_tensor = entity_type_embeddings_batch #.to(torch.float32)

        # Concatenate the tensors along the first dimension
        concat_tensor = torch.cat((cls_tensor, ner_tensor),dim=2).to(device)



        logits = ffn(concat_tensor)
        logits=(torch.max(logits,dim=1).values)

        predictions = torch.argmax(logits, dim=1)
        num_correct += (predictions == labels).sum().item()
        num_total += len(labels)


        if (batch_idx+1) % print_every == 0:
          print("Batch {:} / {:}".format(batch_idx+1,len(dataloader_eval)),"\nLoss:", round(loss.item(),1),"\n")
          print("predictions : ",predictions,"\n","Labels : ",labels,"\n accuarcy : ",num_correct / num_total,"\n")

    eval_accuracy = num_correct / num_total
    loss_of_epoch /= len(dataloader_eval)
    loss_of_epoch_model /= len(dataloader_eval)
    val_model_losses.append(loss_of_epoch_model)
    val_losses.append(loss_of_epoch)

    if eval_accuracy > best_acc:
        print("##########save model########")

        best_acc = eval_accuracy
        torch.save({
            'epoch': epoch+1,
            'model_state_dict': model.state_dict(),
            'ffn_state_dict':ffn.state_dict(),
            'model_optimizer_state_dict': model_optimizer.state_dict(),
            'ffn_optimizer_state_dict': optimizer.state_dict(),
            'loss': loss_of_epoch,
            'accuracy':eval_accuracy
        }, model_path)



    print("\n-------Epoch ", epoch+1,"-------"
      "\n Training Loss FFN:", train_losses[-1],
      "\n Training Loss Model:", train_model_losses[-1],
      "\n validation Loss FFN: ", val_losses[-1],
      "\n validation Loss Model:",val_model_losses[-1] ,
      "\n Time: ",(time.time() - epoch_time),
      "\n Training Accuaracy: ",train_accuracy,
      "\n Evaluation Accuracy:", eval_accuracy,
      "\n-----------------------","\n\n")